{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "max_length = 512\n",
    "\n",
    "training = False\n",
    "pre_trained = False\n",
    "model_epoch = 10 #choose model\n",
    "test_evaluation = False\n",
    "perform = True\n",
    "\n",
    "labels = {'asimov':0,\n",
    "          'non_asimov':1\n",
    "          }\n",
    "print(labels['asimov'])\n",
    "print(labels['non_asimov'])\n",
    "keys_list = list(labels.keys())\n",
    "print(keys_list[0])\n",
    "print(keys_list[1])\n",
    "\n",
    "large_data = True   #states if the data used is the enlarged one\n",
    "large = 'larger_' if large_data else ''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, max_length=512, large=large): #,df\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "        self.asimov_path = f'{large}data/{large}asimov_sentence_dataset.txt'\n",
    "        self.non_asimov_path = f'{large}data/{large}non_asimov_sentence_dataset.txt'\n",
    "        self.dic = {}\n",
    "        self.tot_input = ''\n",
    "        self.input_len = []\n",
    "        self.texts = []\n",
    "        self.token_len = 2 #[CLS] and [SEP]\n",
    "        #two different dataset\n",
    "        for path, label in [(self.asimov_path, 'asimov'), (self.non_asimov_path, 'non_asimov')]:\n",
    "            self.iterative_process(path) #add text, text_len\n",
    "            if label == 'asimov':\n",
    "                print(label)\n",
    "                self.labels = [labels[label] for i in range(self.text_count)]\n",
    "                partial_count = self.text_count\n",
    "            else:\n",
    "                print(label)\n",
    "                for i in range(self.text_count - partial_count):\n",
    "                    self.labels.append(labels[label])\n",
    "        \n",
    "    def iterative_process(self, path):\n",
    "        with open(path, 'r', encoding=\"utf-8\") as data:\n",
    "            data_list = data.readlines()\n",
    "            for i,line in enumerate(data_list):\n",
    "                temp = line\n",
    "                curr_token = self.tokenizer(temp, return_tensors=\"pt\").to(device)\n",
    "                len_curr_token = len(curr_token['attention_mask'][0]) - 2 #ignore [CLS] and [SEP]\n",
    "                if len_curr_token > self.max_length:\n",
    "                    print(len(curr_token['attention_mask'][0]))\n",
    "                self.token_len += len_curr_token\n",
    "                #print(self.token_len)\n",
    "                if self.token_len < (self.max_length - 1):    #I can add the next line\n",
    "                    self.tot_input += temp                    #if we can keep adding we iterate for the next line\n",
    "                    if i == len(data_list)-1:                 #we are at the final line, so we need to add it even if it has not reached the maximum length\n",
    "                        #no need to reset the token len since the dataset is finished\n",
    "                        token = self.tokenizer(self.tot_input, \n",
    "                                            padding='max_length', max_length = max_length,\n",
    "                                            return_tensors=\"pt\").to(device)\n",
    "                        self.texts.append(token)\n",
    "                        if len(token['attention_mask'][0]) > 512:\n",
    "                            print('last adding')\n",
    "                            print(len(token['attention_mask'][0]))\n",
    "                            print(tokenizer.decode(token['input_ids'][0]))\n",
    "                else:   #I can not add other lines, so this is the one I add to the tokenizer\n",
    "                    token = self.tokenizer(self.tot_input, \n",
    "                                            padding='max_length', max_length = max_length,\n",
    "                                            return_tensors=\"pt\").to(device)\n",
    "                    if len(token['attention_mask'][0]) > 512:\n",
    "                        print('normal adding')\n",
    "                        print(len(token['attention_mask'][0]))\n",
    "                        print(tokenizer.decode(token['input_ids'][0]))\n",
    "\n",
    "                    self.texts.append(token)\n",
    "                    self.input_len.append(len(self.tot_input))\n",
    "                    if self.tot_input != temp:\n",
    "                        self.tot_input = temp            #reset tot_input for the next iteration\n",
    "                        self.token_len = len_curr_token  #reset the token len since we are starting over a next sequence \n",
    "                    else: #if the current sequence is too long I discard it\n",
    "                        self.tot_input = ''\n",
    "                        self.token_len = 2\n",
    "            self.text_count = len(self.texts)\n",
    "\n",
    "    \n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, indices) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        # the new labels are the one with the index in the list indices\n",
    "        self.labels = [self.dataset.labels[i] for i in indices]\n",
    "        self.texts = [self.dataset.texts[i] for i in indices]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, list):\n",
    "            return self.dataset[[self.indices[i] for i in idx]]\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation and split of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.1\n",
    "VAL_SIZE = 0.11111111111\n",
    "BATCH_SIZE = 2\n",
    "SEED = 42\n",
    "if training or test_evaluation:\n",
    "\n",
    "    data = Dataset()\n",
    "    \n",
    "\n",
    "    X = range(len(data))\n",
    "    y = data.labels\n",
    "\n",
    "    # generate indices: instead of the actual data we pass in integers\n",
    "    train_indices, test_indices, _, _ = train_test_split(X, y, stratify=y, test_size=TEST_SIZE, random_state=SEED)\n",
    "\n",
    "    mid_train_split = Subset(data, train_indices)\n",
    "\n",
    "    new_X = range(len(mid_train_split))\n",
    "    new_y = mid_train_split.labels\n",
    "\n",
    "    train_indices, val_indices, _, _ = train_test_split(new_X, new_y, stratify=new_y, test_size=VAL_SIZE, random_state=SEED)\n",
    "\n",
    "    # generate subset based on indices\n",
    "    train_split = Subset(data, train_indices)\n",
    "    val_split = Subset(data, val_indices)\n",
    "    test_split = Subset(data, test_indices)\n",
    "\n",
    "    # visualize the dimension of the dataset\n",
    "    print(len(data))\n",
    "    print(len(train_split))\n",
    "    print(len(val_split))\n",
    "    print(len(test_split))\n",
    "    print('############')\n",
    "    print(len(data)/len(data))\n",
    "    print(len(train_split)/len(data))\n",
    "    print(len(val_split)/len(data))\n",
    "    print(len(test_split)/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training or test_evaluation:\n",
    "    with open(f'performance/data/{large}info.txt', 'w') as f:\n",
    "        asimov = 0\n",
    "        non_asimov = 0\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            if data.labels[i] == 0:\n",
    "                asimov+=1\n",
    "            else:\n",
    "                non_asimov+=1\n",
    "\n",
    "        print(f'Samples in all data: asimov are {asimov}, non asimov are {non_asimov}')\n",
    "        a_per = (asimov/len(data))*100\n",
    "        n_a_per = (non_asimov/len(data))*100\n",
    "        print(f'So in data we have {a_per: .2f}% of asimov samples and {n_a_per: .2f}%')\n",
    "\n",
    "        f.write(f'Samples in all data: asimov are {asimov}, non asimov are {non_asimov}\\n')\n",
    "        f.write(f'So in data we have {a_per: .2f}% of asimov samples and {n_a_per: .2f}% of non_asimov\\n')\n",
    "\n",
    "        asimov=0\n",
    "        non_asimov=0\n",
    "        for i in range(len(train_split)):\n",
    "            if train_split.labels[i] == 0:\n",
    "                asimov+=1\n",
    "            else:\n",
    "                non_asimov+=1\n",
    "\n",
    "        print(f'Samples in training data: asimov are {asimov}, non asimov are {non_asimov}')\n",
    "        a_t_per = (asimov/len(train_split))*100\n",
    "        n_a_t_per = (non_asimov/len(train_split))*100\n",
    "        print(f'So in train set we have {a_t_per: .2f}% of asimov samples and {n_a_t_per: .2f}% of non_asimov')\n",
    "\n",
    "        f.write(f'Samples in training data: asimov are {asimov}, non asimov are {non_asimov}\\n')\n",
    "        f.write(f'So in train set we have {a_t_per: .2f}% of asimov samples and {n_a_t_per: .2f}% of non_asimov\\n')\n",
    "\n",
    "        asimov=0\n",
    "        non_asimov=0\n",
    "        for i in range(len(val_split)):\n",
    "            if val_split.labels[i] == 0:\n",
    "                asimov+=1\n",
    "            else:\n",
    "                non_asimov+=1\n",
    "\n",
    "        print(f'Samples in validation data: asimov are {asimov}, non asimov are {non_asimov}')\n",
    "        a_v_per = (asimov/len(val_split))*100\n",
    "        n_a_v_per = (non_asimov/len(val_split))*100\n",
    "        print(f'So in validation set we have {a_v_per: .2f}% of asimov samples and {n_a_v_per: .2f}% of non_asimov')\n",
    "\n",
    "        f.write(f'Samples in validation data: asimov are {asimov}, non asimov are {non_asimov}\\n')\n",
    "        f.write(f'So in validation set we have {a_v_per: .2f}% of asimov samples and {n_a_v_per: .2f}% of non_asimov\\n')\n",
    "\n",
    "\n",
    "        asimov=0\n",
    "        non_asimov=0\n",
    "        for i in range(len(test_split)):\n",
    "            if test_split.labels[i] == 0:\n",
    "                asimov+=1\n",
    "            else:\n",
    "                non_asimov+=1\n",
    "\n",
    "        print(f'Samples in test data: asimov are {asimov}, non asimov are {non_asimov}')\n",
    "        a_test_per = (asimov/len(test_split))*100\n",
    "        n_a_test_per = (non_asimov/len(test_split))*100\n",
    "        print(f'So in test set we have {a_test_per: .2f}% of asimov samples and {n_a_test_per: .2f}% of non_asimov')\n",
    "\n",
    "        f.write(f'Samples in test data: asimov are {asimov}, non asimov are {non_asimov}\\n')\n",
    "        f.write(f'So in test set we have {a_test_per: .2f}% of asimov samples and {n_a_test_per: .2f}% of non_asimov\\n')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs, batch_size=BATCH_SIZE, large=large, pre=pre_trained, st_ep=model_epoch):\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "            \n",
    "            if pre:\n",
    "                epoch_num += st_ep\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            to_print = \\\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                    (total_acc_train: {total_acc_train}) \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_data): .3f}\\\n",
    "                    (total_acc_val: {total_acc_val})'\n",
    "            print(to_print)\n",
    "            with open(f'{large}classifier_model/acc_{epoch_num+1}.txt', 'w') as f:\n",
    "                f.write(to_print)\n",
    "\n",
    "            if (epoch_num+1)%2==0:\n",
    "                torch.save(model.state_dict(), os.path.join(f'{large}classifier_model', f\"{large}classifier-8_1_1-{epoch_num+1}.pt\"),)\n",
    "                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "LR = 1e-6\n",
    "\n",
    "model = BertClassifier()\n",
    "if training:\n",
    "    if pre_trained:\n",
    "        model.load_state_dict(torch.load(f'{large}classifier_model/{large}classifier-8_1_1-{model_epoch}.pt'))\n",
    "        EPOCHS -= model_epoch\n",
    "    train(model, train_split, val_split, LR, EPOCHS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data, large=large):\n",
    "\n",
    "    test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "            pred = output.argmax(dim=1)\n",
    "\n",
    "            for i in range(len(test_label)):\n",
    "                if test_label[i].item() == 1: #non_asimov\n",
    "                    TN+=1\n",
    "                    if pred[i].item() != 1: #predicted asimov instead of non_asimov\n",
    "                        FP+=1\n",
    "                else: #asimov\n",
    "                    TP+=1\n",
    "                    if pred[i].item() != 0: #predicted non_asimov instead of asimov\n",
    "                        FN+=1\n",
    "        \n",
    "        #compute performances\n",
    "        Errors = FP+FN\n",
    "        Accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
    "        Precision = TP/(TP+FP)\n",
    "        Recall = TP/(TP+FN)\n",
    "        F1_score = 2*((Recall * Precision) / (Recall + Precision))\n",
    "    \n",
    "    print(f'Test Accuracy: {Accuracy: .3f}')\n",
    "    print(f'Test Precision: {Precision: .3f}')\n",
    "    print(f'Test Recall: {Recall: .3f}')\n",
    "    print(f'Test F1_score: {F1_score: .3f}')\n",
    "    print(f'Other info:')\n",
    "    print(f'False Positives (predicted asimov instead of non_asimov): {FP}')\n",
    "    print(f'False Negatives (predicted non_asimov instead of asimov): {FN}')\n",
    "    print(f'Errors: {Errors}')\n",
    "    with open(f'performance/{large}info.txt', 'a') as f:\n",
    "        f.write(f'Test Accuracy: {Accuracy: .3f}\\n')\n",
    "        f.write(f'Test Precision: {Precision: .3f}\\n')\n",
    "        f.write(f'Test Recall: {Recall: .3f}\\n')\n",
    "        f.write(f'Test F1_score: {F1_score: .3f}\\n')\n",
    "        f.write(f'Other info: \\n')\n",
    "        f.write(f'False Positives (predicted asimov instead of non_asimov): {FP}\\n')\n",
    "        f.write(f'False Negatives (predicted non_asimov instead of asimov): {FN}\\n')\n",
    "        f.write(f'Errors: {Errors}')\n",
    "\n",
    "if training:\n",
    "    evaluate(model, test_split)\n",
    "    \n",
    "elif test_evaluation:\n",
    "    model.load_state_dict(torch.load(f'{large}classifier_model/{large}classifier-8_1_1-4.pt'))\n",
    "    model.to(device)\n",
    "    evaluate(model, test_split)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not training or not test_evaluation:\n",
    "    #load the model\n",
    "    model.load_state_dict(torch.load(f'{large}classifier_model/{large}classifier-8_1_1-{model_epoch}.pt'))\n",
    "    model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Generated Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_for_evaluation(generated_text, max_length=512):\n",
    "    model_input = []\n",
    "    for i in range(len(generated_text)):\n",
    "        token = tokenizer(generated_text[i], \n",
    "                padding='max_length', max_length = max_length,\n",
    "                return_tensors=\"pt\")\n",
    "        if len(token['input_ids'][0]) > max_length:\n",
    "            temp_array = np.array(token['input_ids'][0])[:max_length-1]\n",
    "            temp = tokenizer.decode(temp_array)\n",
    "\n",
    "            last_occ1 = temp.rfind('.')\n",
    "            last_occ2 = temp.rfind('!')\n",
    "            last_occ3 = temp.rfind('?')\n",
    "\n",
    "            temp = temp[6:max(last_occ1,last_occ2,last_occ3)+1]\n",
    "            cleaned_token = tokenizer(temp, \n",
    "                padding='max_length', max_length = max_length,\n",
    "                return_tensors=\"pt\")\n",
    "            model_input.append(cleaned_token)\n",
    "        else:\n",
    "            model_input.append(token)\n",
    "    return model_input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform:\n",
    "    directory = f'{large}model_outputs'\n",
    "    base_output = os.listdir(directory)\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    all_stories = []\n",
    "    classes = []\n",
    "    for output in base_output:\n",
    "        with open(f'{directory}/{output}', 'r') as f:\n",
    "            text_input = f.read()\n",
    "        text_list = text_input.split('\\n\\n')\n",
    "        for story in text_list:\n",
    "            if 'baseline' in output:\n",
    "                flag = 'non_'\n",
    "                tn += 1\n",
    "            else:\n",
    "                flag = ''\n",
    "                tp += 1\n",
    "            all_stories.append(story)\n",
    "            classes.append(f'{flag}asimov')\n",
    "\n",
    "    model_input = clean_text_for_evaluation(all_stories)\n",
    "\n",
    "    output_classification = []\n",
    "    for test_input in tqdm(model_input):\n",
    "        mask = test_input['attention_mask'].to(device)\n",
    "        input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "        output = model(input_id, mask).to(device)\n",
    "        output_classification.append(keys_list[output.argmax(dim=1)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform:\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    for i in range(len(output_classification)):\n",
    "        if output_classification[i] != classes[i]:\n",
    "            if classes[i] == 'non_asimov': #should be classified as negative but is falsly positive\n",
    "                fp+=1\n",
    "            else: #should be classified as positive but is falsly negative\n",
    "                fn+=1\n",
    "    \n",
    "    #check\n",
    "    print(f'false negatives (non asimov classified as asimov):{fn}')\n",
    "    print(f'false positives (asimov classified as non asimov):{fp}')\n",
    "    errors = fp + fn\n",
    "    print(f'errors: {errors}')\n",
    "\n",
    "    accuracy = (len(output_classification)-errors)/len(output_classification)\n",
    "    print('accuracy:', accuracy)\n",
    "\n",
    "    # precision =  TruePositives / (TruePositives + FalsePositives)\n",
    "    precision = tp / (tp + fp)\n",
    "    # recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "    print('precision', precision)\n",
    "    print('recall', recall)\n",
    "    print('f1_score', f1_score)\n",
    "\n",
    "    print(large)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform:\n",
    "    with open(f'performance/{large}version/Alan woke up.txt', 'w') as f:\n",
    "        f.write(f'accuracy: {accuracy} \\n')\n",
    "        f.write(f'precision: {precision} \\n')\n",
    "        f.write(f'recall: {recall} \\n')\n",
    "        f.write(f'f1_score: {f1_score} \\n')\n",
    "        f.write(f'Other info: \\n')\n",
    "        f.write(f'False negatives (non asimov classified as asimov): {fn} \\n')\n",
    "        f.write(f'False positives (asimov classified as non asimov): {fp} \\n')\n",
    "        f.write(f'errors: {errors}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
