{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_linear_schedule_with_warmup #  AdamW,\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import os\n",
    "from transformers import AutoConfig\n",
    "import re\n",
    "import random\n",
    "from utils.decoding_functions import generate, clean_text\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "SEED = 69\n",
    "random.seed(SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Function\n",
    "The generate function is defined in utils/decoding_functions.\n",
    "It behave differently depending on the input received, it can be behave as:\n",
    "* Top-p Sampling (Nucleus Sampling)\n",
    "* Sample-and-rank\n",
    "* Top-p Sampling-and-rank \n",
    "\n",
    "But first we need to introduce the **Temperature sampling**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Sampling\n",
    "Temperature sampling is a variation of the simple Random Sampling using the parameter $T$ in the following way.\n",
    "\n",
    "Temperature $T > 0$ is a hyper-parameter that regulates the probability distribution $p_i$ of the next\n",
    "token during decoding. We divide the logits $z_i$ by $T$ (calling it $T_z$) before computing the “softmax” as in Hinton et al. (2015):\n",
    "$$\n",
    "Tz = z/T\\\\\n",
    "\\\\\n",
    "p_i = \\frac{\\exp(Tz_i)}{\\sum_j{\\exp(Tz_j)}}\n",
    "$$\n",
    "So the formula looks like:\n",
    "$$\n",
    "p_i = \\frac{\\exp(z_i/T)}{\\sum_j{\\exp(z_j/T)}}\n",
    "$$\n",
    "$T = 1$ yields the unmodified distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-p Sampling\n",
    "Generate function receiving in input just the *top-p value*.\n",
    "\n",
    "In this approach, we sum up all the probabilities, sorted in descending order, that are present until the total sum (the cumulative distribution function) is above an adjustable hyperparameter, p, which is normally set between 0.7 and 0.9.\n",
    "\n",
    "Once the CDF is formed, we eliminate everything that falls outside of our p by setting it to -Infinity. Note that as we’re doing this by summing the highest probability selections first, it’s possible that if there’s a few high probability choices, they’ll be the only ones present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "#generated_text = generate(model, tokenizer, top_p=top_p, prompt='Alan woke up', device=device, entry_count=entry_count, entry_length=entry_length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Top-p Sampling\n",
    "Generate function receiving in input both *temperature* with $T\\neq1$ and *top-p value*.\n",
    "\n",
    "It is a variation of Top-p Sampling in which the probabilities, before being ordered, are modified by the Temperature value. If $T=1$ it is equal to Top-p Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "#generated_text = generate(model, tokenizer, temperature=temperature, top_p=top_p, prompt='Alan woke up', device=device, entry_count=entry_count, entry_length=entry_length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample-and-Rank\n",
    "Generate function receiving in input both the *temperature* and the *number of samples*.\n",
    "\n",
    "Sample-and-rank, works as follows:\n",
    "* Sample N independent candidate responses using plain random sampling with temperature $T$.\n",
    "* Second, we select the candidate response with the highest probability to use as the final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "#generate(model, tokenizer, temperature=temperature, num_samples=num_samples, prompt='Alan woke up', device=device, entry_count=entry_count, entry_length=entry_length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-p Sample-and-rank\n",
    "Generate function receiving in input **all** the values seen before: *temperature*, *number of samples* and *top-p value*.\n",
    "\n",
    "It works as Sample and Rank but exploit the nucleus sampling approach to select the candidates for the final outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "#generate(model, tokenizer, temperature=temperature, num_samples=num_samples, top-p, temperature=temperature, prompt='Alan woke up', device=device, entry_count=entry_count, entry_length=entry_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine-tuning\n",
    "gpt2_type = 'gpt2' #fixed\n",
    "\n",
    "entry_count = 10\n",
    "entry_length = 512\n",
    "\n",
    "large_data = True   #states if the data used is the enlarged one\n",
    "large = 'larger_' if large_data else ''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate texts from the baseline model using Temperature Top-P sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(gpt2_type)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "print('total number of tests: 36')\n",
    "tot = 0\n",
    "just_once = 0\n",
    "model = GPT2LMHeadModel.from_pretrained(gpt2_type)\n",
    "#for i in range(6):\n",
    "i=00\n",
    "for temperature in [0.88, 1.]:\n",
    "        for top_p in [0.7,0.8,0.9]:\n",
    "            generated_text = generate(model, tokenizer, temperature=temperature, top_p=top_p, prompt='Alan woke up', device=device, entry_count=entry_count, entry_length=entry_length)\n",
    "            cleaned_text = clean_text(generated_text)\n",
    "            with open(f'{large}model_outputs/baseline_startoftext_top_p{top_p}_T{temperature}_el{entry_length}-{i}.txt','w', encoding='utf-8') as f:\n",
    "                f.write('\\n\\n'.join(cleaned_text))\n",
    "            tot+=1\n",
    "            print(f'completed test: {tot}/36')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate texts from the different fine-tuned models using Temperature Top-P sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(gpt2_type)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "print('total number of tests: 36')\n",
    "tot = 0\n",
    "just_once = 0\n",
    "model = GPT2LMHeadModel(config)\n",
    "model.load_state_dict(torch.load(f'{large}model/final_startoftext-20.pt'))\n",
    "#for i in range(6):\n",
    "i=00\n",
    "for temperature in [0.88, 1.]:\n",
    "    for top_p in [0.7,0.8,0.9]:\n",
    "        generated_text = generate(model, tokenizer, temperature=temperature, top_p=top_p, prompt='Alan woke up', device=device, entry_count=entry_count, entry_length=entry_length)\n",
    "        cleaned_text = clean_text(generated_text)\n",
    "        with open(f'{large}model_outputs/final_startoftext-20_top_p{top_p}_T{temperature}_el{entry_length}-{i}.txt','w', encoding='utf-8') as f:\n",
    "            f.write('\\n\\n'.join(cleaned_text))\n",
    "        tot+=1\n",
    "        print(f'completed test: {tot}/36')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32aaecebd078ebf0fad58c11ce872e322c9fff2b8f0b0f5a9c84d62363eabb98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
