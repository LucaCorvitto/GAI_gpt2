{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install epub2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from epub2txt import epub2txt\n",
    "import os\n",
    "import nltk.data\n",
    "\n",
    "asimov = True #states if building the asimov or non_asimov dataset\n",
    "large_data = True\n",
    "flag = 'asimov_' if asimov else 'non_asimov_'\n",
    "large = 'larger_' if large_data else ''\n",
    "\n",
    "epub_dir = f'{large}data/{flag}epub'\n",
    "text_dir = f'{large}data/{flag}texts'\n",
    "ref_text_dir = f'{large}data/{flag}refined_texts'\n",
    "sen_dir = f'{large}data/{flag}sentences'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Convert epub to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_list = os.listdir(epub_dir)\n",
    "for book in book_list:\n",
    "    filepath = f\"{epub_dir}/{book}\"\n",
    "    name = os.path.split(filepath)[1].split('.')[0]\n",
    "    print(name)\n",
    "    res = epub2txt(filepath)\n",
    "    with open(f'{text_dir}/{name}.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Manual Texts Refinement\n",
    "Since the complexity required to detect which parts of the texts had to be removed, this step was done manually. Unfortunally, this step is fundamental because it secures that the texts are free of useless information that represents noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no code for this part"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Divide each file in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_list = os.listdir(ref_text_dir)\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "for book in book_list:\n",
    "    sentence_data = \"\"\n",
    "    with open(f\"{ref_text_dir}/{book}\", 'r', encoding=\"utf-8\") as fp: \n",
    "        temp = fp.read()\n",
    "        sentence_data += '\\n'.join(tokenizer.tokenize(temp))\n",
    "    with open(f\"{sen_dir}/sentence_{book}\", 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(sentence_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Unify all the files in a big sentence dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_list = os.listdir(sen_dir)\n",
    "complete_data = \"\"\n",
    "with open(f'{large}data/{large}{flag}sentence_dataset.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    for book in book_list:\n",
    "        with open(f'{sen_dir}/{book}', 'r', encoding=\"utf-8\") as text:\n",
    "            complete_data += text.read()\n",
    "        complete_data += '\\n\\n\\n'\n",
    "    f.write(complete_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32aaecebd078ebf0fad58c11ce872e322c9fff2b8f0b0f5a9c84d62363eabb98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
