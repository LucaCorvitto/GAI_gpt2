{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "\n",
    "SEED = 69\n",
    "random.seed(SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_seq_length -> gpt2=768; gpt2-medium=1024; gpt2-large=1420; gpt2-xl=1600\n",
    "\n",
    "#fine-tuning\n",
    "gpt2_type = 'gpt2'\n",
    "max_seq_length = 768\n",
    "control_code = 'startoftext'\n",
    "large_data = True\n",
    "large = 'larger_' if large_data else ''\n",
    "epochs=20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding sentences to the textual input until the setted max_length is reached. In this way each input contains more than one sentence and each of them is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealAsimov(Dataset):\n",
    "    def __init__(self, control_code = control_code, gpt2_type=gpt2_type, max_length=1024, larger=''):\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.text = []\n",
    "        self.input_len = []\n",
    "        self.tot_input = ''\n",
    "        self.discarded = ''\n",
    "\n",
    "        self.file = f'{larger}data/{larger}asimov_sentence_dataset.txt'\n",
    "        \n",
    "        with open(self.file, 'r', encoding=\"utf-8\") as data:\n",
    "            data_list = data.readlines()\n",
    "            for i,line in enumerate(data_list):\n",
    "                temp = line + self.discarded\n",
    "                self.discarded = ''\n",
    "                if len(self.tot_input + temp) < max_length: #I can add the current line to the input\n",
    "                    self.tot_input += temp\n",
    "                    if i == len(data_list)-1: # if we are at the final line\n",
    "                        self.text.append(torch.tensor(\n",
    "                            self.tokenizer.encode(f\"<|{control_code}|>{self.tot_input[:max_length]}<|endoftext|>\")\n",
    "                        ))\n",
    "                else: #Adding the current sentence would result in an input longer than the max\n",
    "                    self.text.append(torch.tensor(\n",
    "                            self.tokenizer.encode(f\"<|{control_code}|>{self.tot_input[:max_length]}<|endoftext|>\")\n",
    "                        ))\n",
    "                    self.input_len.append(len(self.tot_input))\n",
    "                    self.tot_input = ''\n",
    "                    self.discarded = temp #keep in memory the current sentence\n",
    "            self.text_count = len(self.text)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.text_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.text[item]\n",
    "    \n",
    "\n",
    "dataset = RealAsimov(control_code=control_code, gpt2_type=gpt2_type, larger=large) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "model = GPT2LMHeadModel.from_pretrained(gpt2_type)\n",
    "\n",
    "#Accumulated batch size (since GPT2 is so big)\n",
    "def pack_tensor(new_tensor, packed_tensor, max_seq_len): #-> (input_tensor, carry_on, remainder)\n",
    "    if packed_tensor is None: #first iteration\n",
    "        return new_tensor, True, None   #input_tensor=new_tensor; carry_on=True; remainder=None\n",
    "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len: #new tensor is too big and can not be packed together with the previous one\n",
    "        return packed_tensor, False, new_tensor #input_tensor=packed_tensor; carry_on=False; remainder=new_tensor\n",
    "    else: #can be packed\n",
    "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1) #concatenate the tensors\n",
    "        return packed_tensor, True, None    #input_tensor=packed_tensor; carry_on=True; remainder=none"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset,\n",
    "    model,\n",
    "    batch_size=16,\n",
    "    epochs=3,\n",
    "    lr=2e-5,\n",
    "    max_seq_len=max_seq_length,\n",
    "    warmup_steps=5000,\n",
    "    device=\"cuda\",\n",
    "    output_dir=\"model\",\n",
    "    output_prefix=control_code,\n",
    "    save_model_on_epoch=False,\n",
    "):\n",
    "\n",
    "    device=torch.device(\"cuda\")\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=False) #in order to keep sentences order\n",
    "    loss=0\n",
    "    accumulating_batch_count = 0\n",
    "    input_tensor = None\n",
    "\n",
    "    number_of_training_steps = len([b for b in train_dataloader])\n",
    "    for epoch in range(epochs):\n",
    "        batch_pbar = tqdm(enumerate(train_dataloader), total=number_of_training_steps)\n",
    "        for idx, entry in batch_pbar:\n",
    "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, max_seq_len)\n",
    "\n",
    "            if carry_on and idx != len(train_dataloader) - 1:\n",
    "                continue\n",
    "\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            outputs = model(input_tensor, labels=input_tensor)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "\n",
    "            if (accumulating_batch_count % batch_size) == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "\n",
    "            accumulating_batch_count += 1\n",
    "            input_tensor = remainder # to avoid loss of entries we reset the input tensor to the current entry we did not use in the packed tensor\n",
    "        \n",
    "        if save_model_on_epoch:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{large}final_{output_prefix}-{epoch}.pt\"),\n",
    "            )\n",
    "\n",
    "        if (epoch+1)%2 == 0:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{large}final_{output_prefix}-{epoch+1}.pt\"),\n",
    "            )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train / fine tune\n",
    "model = train(dataset, model, output_dir=f\"{large}model\", epochs=epochs, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32aaecebd078ebf0fad58c11ce872e322c9fff2b8f0b0f5a9c84d62363eabb98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
